\subsection{Almacenamiento de Datos}

\subsubsection {Formatos de Almacenamiento}

Los formatos de datos son un componente fundamental en cualquier arquitectura de sistemas de información moderna, 
ya que determinan no solo cómo se almacena la información, sino también cómo se procesa, transmite y analiza. 
La elección adecuada del formato de datos puede tener un impacto significativo en el rendimiento, 
la escalabilidad y la eficiencia del sistema en su conjunto. Para un sistema de Big Data, 
donde se manejan grandes volúmenes de información, la importancia de estos formatos se magnifica, 
ya que pueden significar la diferencia entre un sistema eficiente y uno que consume recursos excesivos. 
Además, los formatos de datos actúan como un lenguaje común entre diferentes componentes, 
facilitando la interoperabilidad y la integración de tecnologías.

\subsubsection{Formatos Orientados a Filas}
Los formatos orientados a filas representan la forma tradicional de almacenamiento de datos, 
donde cada registro se almacena de manera secuencial. Este enfoque ha sido la base de los sistemas 
de gestión de bases de datos durante décadas y sigue siendo crucial en muchos escenarios.

\begin{itemize}
    \item Los registros completos se almacenan de manera contigua en disco
    \item Cada fila contiene todos los campos de un registro
    \item Optimizado para acceder a registros completos
    \item Los nuevos registros se añaden secuencialmente de forma eficiente
    \item Óptimo cuando las consultan necesitan todos los campos
    \item Fácil modificación de registros individuales
    \item Debe leer datos innecesarios cuando solo se necesitan algunas columnas
    \item Los datos heterogéneos juntos reducen la efectividad de los métodos de compresión
    \item Menos eficiente para análisis de columnas específicas
\end{itemize}

\subsubsection{Formatos Orientados a Columnas}

Los formatos de almacenamiento columnar representan un paradigma fundamental en el manejo de datos masivos, 
especialmente en entornos analíticos. A diferencia del almacenamiento tradicional orientado a filas, 
donde los registros se almacenan secuencialmente, el almacenamiento columnar organiza los datos por columnas, 
lo que ofrece ventajas significativas en ciertos escenarios.

\begin{itemize}
    \item En lugar de almacenar registros completos de manera contigua, los datos se organizan por columnas
    \item Cada columna se almacena en bloques separados de memoria o disco
    \item Los valores similares se almacenan juntos, mejorando la compresión
    \item Los datos similares almacenados juntos permiten mayores tasas de compresión
    \item Solo se leen las columnas necesarias para una consulta
    \item Facilita operaciones como SUM, AVG, COUNT sobre columnas específicas
    \item Permite procesamiento eficiente de datos en hardware
\end{itemize}

\subsubsection{Comparativa con Almacenamiento por Filas:}

Consideremos una tabla simple de usuarios:

\textbf{Formato por Filas:}
\begin{verbatim}
[ID1, "Juan", 25] -> [ID2, "Ana", 30] -> [ID3, "Pedro", 28]
\end{verbatim}

\textbf{Formato Columnar:}
\begin{verbatim}
IDs:    [ID1 -> ID2 -> ID3]
Nombres: ["Juan" -> "Ana" -> "Pedro"]
Edades:  [25 -> 30 -> 28]
\end{verbatim}

\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspecto} & \textbf{Formato Columnar} & \textbf{Formato por Filas} \\
\hline
Lectura parcial & Muy eficiente & Menos eficiente \\
Inserción de registros & Más lenta & Más rápida \\
Compresión & Alta & Moderada \\
Consultas analíticas & Excelente & Regular \\
Consultas transaccionales & Regular & Excelente \\
\hline
\end{tabular}

\subsubsection{Formatos Específicos}

\paragraph{JSON (JavaScript Object Notation)} 
se ha convertido en el estándar de facto para el intercambio 
de datos en aplicaciones modernas, especialmente en entornos Web y APIs. Su popularidad se debe a su 
simplicidad, legibilidad humana y amplia compatibilidad con prácticamente todos los lenguajes de 
programación. A pesar de no ser el más eficiente en términos de espacio y rendimiento (ya que es un formato basado en filas), 
su flexibilidad para representar datos estructurados y semiestructurados lo hace invaluable en sistemas donde la 
interoperabilidad y la facilidad de desarrollo son prioritarias. Es particularmente útil en aplicaciones
donde las transacciones individuales y la flexibilidad del esquema son más importantes que la 
eficiencia en el procesamiento de grandes volúmenes de datos.

\paragraph{Apache AVRO} 
destaca como un formato de serialización de datos binario que combina la 
eficiencia del almacenamiento binario con la flexibilidad de esquemas evolutivos. Su característica 
más distintiva es su capacidad para manejar cambios en el esquema de datos a lo largo del tiempo sin 
requerir cambios en el código o reescritura de datos existentes. Para esto, AVRO almacena el esquema junto con 
los datos, lo que permite una deserialización precisa y eficiente. Es especialmente valioso en 
sistemas de mensajería y streaming de datos, donde la evolución del esquema y la 
eficiencia en la transmisión son cruciales. Su formato binario compacto y su capacidad de compresión 
lo hacen ideal para sistemas distribuidos donde el ancho de banda y el almacenamiento son 
consideraciones importantes.

\paragraph{Apache Parquet} 
se ha establecido como el formato columnar dominante en el ecosistema de 
Big Data, especialmente para cargas de trabajo analíticas. Su diseño columnar permite una compresión 
altamente eficiente y un muy buen rendimiento en consultas que involucran solo un subconjunto de 
columnas. Parquet destaca particularmente en escenarios de análisis de datos, 
donde su capacidad para manejar esquemas complejos anidados y su integración con 
casi todas las herramientas lo hacen indispensable. La adopción generalizada de Parquet en la 
industria, lo ha convertido en el estándar de facto para almacenamiento de datos analíticos.

\newpage
\paragraph{Optimized Row Columnar (ORC)} 
inicialmente fué desarrollado para optimizar Hive, 
y aunque ofrece excelentes capacidades de compresión y rendimiento en consultas, su relevancia 
ha disminuido significativamente en los últimos años frente a Parquet. Aunque ORC sigue siendo 
relevante en sistemas legacy y específicos de Hive, la tendencia de la industria se ha movido 
claramente hacia Parquet como el formato columnar preferido para análisis de datos a gran escala.

\subsubsection{Almacenamiento de Objetos en la Nube}

El almacenamiento de objetos en la nube constituye un paradigma de almacenamiento donde los datos se organizan y gestionan como objetos independientes dentro de una estructura plana.
Cada objeto almacenado comprende tres elementos fundamentales: los datos en sí mismos, un conjunto extenso de metadatos que describen y categorizan la información, 
y un identificador único global que permite su localización y recuperación. 
Dicho paradigma, se caracteriza por su naturaleza distribuida y su capacidad para manejar tanto datos estructurados como no estructurados, 
permitiendo almacenar desde documentos, archivos multimedia y hasta flujos de datos en tiempo real.

Los sistemas de almacenamiento de objetos se destaca por su capacidad para satisfacer las demandas contemporáneas de procesamiento de datos a gran escala.
Como por ejemplo, ofrece una escalabilidad prácticamente ilimitada, pudiendo crecer según las necesidades sin preocuparse por restricciones de capacidad.
Por otro lado, la durabilidad y disponibilidad de los datos se garantiza a través de la replicación automática en múltiples ubicaciones de forma automática y transparente.
Adicionalmente, proveen APIs normalmente basadas en el protocolo HTTP que permite acceder de forma interoperable y estandarizada a los recursos almacenados

El uso de estos sistemas tambien conlleva sus propios desafíos. El más importante puede ser la latencia; pero también deben los grados de consistencia que ofrecen.

\clearpage

\subsubsection{Formato de Tabla Analítica}

Los formatos de tabla analítica son tecnologías diseñadas para resolver los desafíos del manejo de datos
a gran escala. Surgen como respuesta a las limitaciones de los formatos de archivo tradicionales como Parquet y ORC
cuando se trabaja con ellos en la nube. 

Las características más importantes que aporta un formato de tabla analítica son:

\begin{itemize}
    \item Proveen abstracciones sobre la metadata de archivos
    \item Permiten el uso de tablas con semántica SQL y evolución de esquema en las mismas
    \item Transacciones ACID
    \item Actualizaciones y Borrados
    \item Optimización de datos para mejoras de rendimiento
    \item Compatibilidad con múltiples motores de procesamiento
    \item Control de versiones en los datos
\end{itemize}

Los formatos de tablas analíticas dan a sus sistemas subyacentes estas características a través de diferentes mecanismos y estrategias. 
La principal es la gestión de metadatos, ya que implementan estructuras de datos altamente optimizadas que permiten rastrear 
eficientemente los archivos y sus cambios; mientras mantienen un historial detallado de transacciones que garantiza la consistencia de los datos, 
complementando con estrategias efectivas de particionamiento y organización. 

Para la optimización del rendimiento, estos formatos emplean diversas técnicas como la compactación automática de archivos, 
estrategias de caching de datos para acceso rápido, utilización de formatos de archivo columnares como Parquet u ORC, 
y la incorporación de capacidades avanzadas de indexación y filtrado optimizado. 
La consistencia de los datos se garantiza mediante la implementación de transacciones ACID completas, 
que proporcionan un sólido aislamiento entre operaciones de lectura y escritura, manejan conflictos de manera automática 
y aseguran la consistencia en escenarios de operaciones concurrentes. 
En cuanto a la evolución y mantenimiento, estos formatos facilitan cambios de esquema sin interrupciones en el servicio. 

Todas estas mecanismos trabajan en conjunto para proporcionar una solución completa para el manejo de datos a escala masiva,
que como subproducto permite tratar la escritura de los archivos como un canal de mensajes sobre el que se puede hacer streaming.

Los exponentes más importantes de estos formatos son: Delta Table, Apache Iceberg y Apache Hudi

\paragraph{Delta Lake}es un sistema de almacenamiento de datos diseñado por Databricks que utiliza archivos Parquet como base, 
organizándolos en una estructura de directorios con dos componentes principales: 
los archivos de datos en formato Parquet y el directorio {\_delta\_log} para metadatos y registro de transacciones. 
Esta arquitectura mantiene las ventajas de Parquet mientras añade capacidades transaccionales y de control de versiones, 
implementando un sistema de checkpoints para optimizar el rendimiento y un manejo de concurrencia que combina control optimista 
con serialización de escrituras.

Las características fundamentales de Delta Lake incluyen transacciones ACID completas, 
capacidad de acceso a versiones anteriores, evolución de esquema controlada, 
operaciones de Merge sofisticadas y optimización automática de datos mediante compactación de archivos y mantenimiento de estadísticas. 
El sistema también proporciona soporte para procesamiento de datos en tiempo real con semántica exactly-once 
y una integración robusta principalmente con Spark.

\paragraph{Apache Hudi}desarrollado inicialmente por Uber, es una plataforma enfocada en crear una plataforma analítica transaccional,
disponibilizando dos tipos principales de formatos de tablas: 
Copy On Write (optimizado para lecturas) y Merge On Read (optimizado para escritura). 

Utiliza una Timeline para gestionar metadatos y registrar cronológicamente todas las acciones, 
implementando un sistema de indexación que permite optimizar operaciones y facilitar búsquedas rápidas, 
además maneja control de concurrencia optimista que mantiene lecturas sin bloqueos mientras serializa escrituras.
Las características principales de Hudi incluyen procesamiento incremental para manejar streams de datos, 
gestión sofisticada de registros individuales con versionado a nivel de registro, borrado lógico y evolución de esquemas.
También proporciona garantías de consistencia con transacciones ACID y semántica exactly-once, 
ofreciendo buena integración con motores de procesamiento como Spark y Flink.

Hudi, al ser una plataforma, no ofrece solo su formato de tabla analítica, 
sino también "Table Services" que son componentes computacionales que permiten la optimización como compactación automática 
y limpieza de almacenamiento. 

\newpage
\paragraph{Apache Iceberg}diseñado inicialmente por Netflix, implementa una arquitectura de almacenamiento que se caracteriza 
por un modelo de metadatos enfocado en la evolución del esquema. 

Utiliza una estructura jerárquica que separa completamente los metadatos de los datos, 
implementando control de versiones basado en snapshots atómicos e inmutables, 
donde cada snapshot representa un punto en el tiempo de la tabla y contiene referencias a todos los archivos de datos válidos para esa versión, 
permitiendo operaciones concurrentes sin necesidad de bloqueos pesados.

Entre sus características principales destacan una gestión de esquemas flexible que permite evolucionar tanto el esquema 
como la estrategia de particionamiento sin reescribir datos, 
una optimización de consultas avanzada basada en estadísticas detalladas a nivel de columna y archivos, 
y un sistema de control de concurrencia optimista. 

Además, disponibiliza herramientas de mantenimiento como expiración de snapshots, compactación de archivos 
y reescritura de datos para optimización física, 
junto con una robusta integración con diferentes motores de procesamiento como Spark y Flink.

\newpage

\subsubsection{Catálogos de Metadatos}
Los catálogos de metadatos actúan como un registro centralizado y organizado de toda la información sobre las tablas y conjuntos de datos en un sistema. 
Son una capa de abstracción que mantiene información crítica sobre la estructura, ubicación, esquema, particiones, historial de versiones y estadísticas de los datos, 
permitiendo una gestión eficiente y un acceso optimizado a los mismos.

Son necesarios para que sistemas externos conozcan la ubicación y la estructura de los datos almacenados. 

\paragraph{Hive Metastore} es la implementación más adoptada y el estándar de estos catálogos. Opera como un servicio centralizado que almacena la información sobre las estructuras de datos,
típicamente usando una base de datos relacional como base (por ejemplo PostgreSQL o MySQL). Sin embargo, puede presentar problemas con operaciones concurrentes complejas. 
Sin embargo, se pueden implementar capas de abstracción que permitan manejar estos escenarios, y generalmente se prefiere utilizar este sistema ya que está ampliamente probado.

\paragraph{Project Nessie} es otra implementación de catálogo de datos, aunque con un enfoque moderno, aplicando conceptos de control de versiones a los datos; similar a como lo hace git. 
No intenta mantener compatibilidad con Hive Metastore, sino que ofrece un nuevo paradigma de gestión de datos, por lo que puede implementar características más complejas. 
Sin embargo, esto también significa que requiere más esfuerzo en integración con herramientas existentes ya que su ecosistema aún no está tan desarrollado.

\paragraph{Apache Polaris} es un proyecto relativamente nuevo que busca estandarizar y modernizar la gestión de metadatos. Es un esfuerzo para estandarizar y unificar los catálogos de metadatos.
De esta manera, Apache Polaris busca ser una solución neutral y agnóstica a los distintos proveedores de catálogos.  

\newpage

\subsubsection{Bases de Datos Analíticas}

Las bases de datos analíticas, también conocidas como bases de datos OLAP (Online Analytical Processing) orientadas a tiempo real, 
son sistemas especializados diseñados para procesar y analizar grandes volúmenes de datos con énfasis particular en consultas complejas y agregaciones. 
Estos sistemas están arquitecturalmente optimizados para procesar rápidamente consultas que involucran múltiples dimensiones y métricas sobre conjuntos masivos de datos, 
permitiendo análisis en tiempo real o casi real. 

Estas bases de datos se distinguen por su capacidad de manejar cargas de trabajo analíticas complejas mientras mantienen latencias bajas y consistentes; 
especializandose en resultados en segundos o milisegundos.Esta característica se debe a su arquitectura orientada específicamente al análisis, 
que contrasta con los sistemas diseñados primariamente para transacciones (OLTP) o almacenamiento general de datos.

Por diseño permiten un análisis multidimensional eficiente, soportan alta concurrencia de usuarios, y pueden integrarse efectivamente con fuentes de datos en streaming.
Además, su diseño orientado a columnas permite una compresión más eficiente y mejor rendimiento en consultas analíticas que típicamente involucran solo un subconjunto de ellas. 
Proporcionando capacidades avanzadas de agregación y pueden manejar eficientemente tanto datos históricos como en tiempo real.

Ninguno de estos beneficios vienen sin sus propios desafios, ya que se requiere una cuidadosa planificacion, operación y mantenimiento para ,amtemer su rendimiento.
Además, más que nunca, es necesario organizar correctamente los indices y los esquemas de particionamiento son claves.

Ejemplos de estos sistemas son:

\paragraph{Apache Druid}
es una base de datos analítica distribuida diseñada principalmente para análisis en tiempo real de grandes volúmenes de datos de series temporales. 
Su arquitectura se distingue por su capacidad de ingesta en tiempo real combinada con consultas de baja latencia, 
utilizando un modelo de almacenamiento columnar híbrido que combina datos en memoria con almacenamiento en disco. 

\paragraph{Apache Pinot} 
fué desarrollado incialmente por LinkedIn y se enfoca en proporcionar análisis en tiempo real con latencias extremadamente bajas, 
incluso en escenarios de alta concurrencia de usuarios. 
Su arquitectura está optimizada para consultas de lectura masivas y paralelas. 
Además, destaca por su modelo de consistencia eventual y su capacidad para manejar esquemas dinámicos.

\paragraph{Apache Doris} 
inicialmente fue desarrollado por Baidu, integra capacidades de almacenamiento columnar MPP (Procesamiento Paralelo Masivo) con funcionalidades OLAP, 
ofreciendo una solución más cercana a una base de datos tradicional pero con capacidades analíticas avanzadas. 
Su arquitectura es más simple en comparación con Druid y Pinot, lo que facilita la operación y mantenimiento, manteniendo un buen rendimiento para consultas analíticas.

\newpage